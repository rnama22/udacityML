{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  emotion                                             pixels  \\\n",
       "0           0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...   \n",
       "1           1        0  151 150 147 155 148 133 111 140 170 174 182 15...   \n",
       "2           2        2  231 212 156 164 174 138 161 173 182 200 106 38...   \n",
       "3           3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...   \n",
       "4           4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...   \n",
       "\n",
       "      Usage  \n",
       "0  Training  \n",
       "1  Training  \n",
       "2  Training  \n",
       "3  Training  \n",
       "4  Training  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '/Users/kcl759/Downloads/fer2013_prepped.csv'\n",
    "image_size=(48,48)\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fer2013():\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'),image_size)\n",
    "        faces.append(face.astype('float32'))\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
    "    return faces, emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "faces, emotions = load_fer2013()\n",
    "faces = preprocess_input(faces)\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# parameters\n",
    "batch_size = 32\n",
    "num_epochs = 110\n",
    "input_shape = (48, 48, 1)\n",
    "verbose = 1\n",
    "num_classes = 6\n",
    "patience = 50\n",
    "base_path = 'models/'\n",
    "l2_regularization=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 46, 46, 8)    72          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 46, 46, 8)    32          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 46, 46, 8)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 44, 44, 8)    576         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 44, 44, 8)    32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 44, 44, 8)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 44, 44, 16)   200         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 44, 44, 16)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 44, 44, 16)   400         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 22, 22, 16)   128         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 22, 22, 16)   64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_9[0][0]            \n",
      "                                                                 batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 22, 22, 32)   656         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 22, 22, 32)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 22, 22, 32)   1312        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 11, 11, 32)   512         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 11, 11, 32)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 11, 11, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 11, 11, 32)   0           max_pooling2d_10[0][0]           \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 11, 11, 64)   2336        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 11, 11, 64)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 11, 11, 64)   4672        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 6, 6, 64)     2048        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 6, 6, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 6, 6, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 6, 64)     0           max_pooling2d_11[0][0]           \n",
      "                                                                 batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 6, 6, 128)    8768        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 6, 6, 128)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 6, 6, 128)    17536       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 3, 3, 128)    8192        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 3, 3, 128)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 3, 3, 128)    512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 3, 3, 128)    0           max_pooling2d_12[0][0]           \n",
      "                                                                 batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 3, 3, 6)      6918        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 6)            0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 6)            0           global_average_pooling2d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 57,270\n",
      "Trainable params: 55,798\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "898/897 [==============================] - 188s 209ms/step - loss: 1.7397 - acc: 0.3214 - val_loss: 1.5338 - val_acc: 0.4186\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.53379, saving model to models/_mini_XCEPTION.01-0.42.hdf5\n",
      "Epoch 2/110\n",
      "898/897 [==============================] - 197s 219ms/step - loss: 1.4841 - acc: 0.4293 - val_loss: 1.6731 - val_acc: 0.3838\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.53379\n",
      "Epoch 3/110\n",
      "898/897 [==============================] - 188s 209ms/step - loss: 1.3696 - acc: 0.4759 - val_loss: 1.3421 - val_acc: 0.4907\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53379 to 1.34211, saving model to models/_mini_XCEPTION.03-0.49.hdf5\n",
      "Epoch 4/110\n",
      "898/897 [==============================] - 178s 198ms/step - loss: 1.2993 - acc: 0.4983 - val_loss: 1.2591 - val_acc: 0.5228\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.34211 to 1.25912, saving model to models/_mini_XCEPTION.04-0.52.hdf5\n",
      "Epoch 5/110\n",
      "898/897 [==============================] - 182s 203ms/step - loss: 1.2515 - acc: 0.5199 - val_loss: 1.2434 - val_acc: 0.5231\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.25912 to 1.24340, saving model to models/_mini_XCEPTION.05-0.52.hdf5\n",
      "Epoch 6/110\n",
      "898/897 [==============================] - 179s 199ms/step - loss: 1.2215 - acc: 0.5302 - val_loss: 1.2935 - val_acc: 0.5092\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.24340\n",
      "Epoch 7/110\n",
      "898/897 [==============================] - 178s 198ms/step - loss: 1.1940 - acc: 0.5389 - val_loss: 1.3195 - val_acc: 0.5123\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.24340\n",
      "Epoch 8/110\n",
      "898/897 [==============================] - 186s 207ms/step - loss: 1.1679 - acc: 0.5490 - val_loss: 1.1962 - val_acc: 0.5407\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.24340 to 1.19617, saving model to models/_mini_XCEPTION.08-0.54.hdf5\n",
      "Epoch 9/110\n",
      "898/897 [==============================] - 180s 200ms/step - loss: 1.1528 - acc: 0.5564 - val_loss: 1.1478 - val_acc: 0.5575\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.19617 to 1.14777, saving model to models/_mini_XCEPTION.09-0.56.hdf5\n",
      "Epoch 10/110\n",
      "898/897 [==============================] - 174s 194ms/step - loss: 1.1340 - acc: 0.5648 - val_loss: 1.1682 - val_acc: 0.5510\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.14777\n",
      "Epoch 11/110\n",
      "898/897 [==============================] - 804s 895ms/step - loss: 1.1248 - acc: 0.5698 - val_loss: 1.4340 - val_acc: 0.4692\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.14777\n",
      "Epoch 12/110\n",
      "898/897 [==============================] - 181s 201ms/step - loss: 1.1088 - acc: 0.5752 - val_loss: 1.1234 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.14777 to 1.12339, saving model to models/_mini_XCEPTION.12-0.57.hdf5\n",
      "Epoch 13/110\n",
      "898/897 [==============================] - 185s 206ms/step - loss: 1.0925 - acc: 0.5811 - val_loss: 1.1288 - val_acc: 0.5699\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.12339\n",
      "Epoch 14/110\n",
      "898/897 [==============================] - 178s 198ms/step - loss: 1.0868 - acc: 0.5837 - val_loss: 1.1329 - val_acc: 0.5674\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.12339\n",
      "Epoch 15/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 1.0745 - acc: 0.5899 - val_loss: 1.0804 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.12339 to 1.08037, saving model to models/_mini_XCEPTION.15-0.59.hdf5\n",
      "Epoch 16/110\n",
      "898/897 [==============================] - 177s 197ms/step - loss: 1.0641 - acc: 0.5956 - val_loss: 1.1067 - val_acc: 0.5752\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.08037\n",
      "Epoch 17/110\n",
      "898/897 [==============================] - 177s 197ms/step - loss: 1.0589 - acc: 0.5970 - val_loss: 1.1621 - val_acc: 0.5631\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.08037\n",
      "Epoch 18/110\n",
      "898/897 [==============================] - 184s 205ms/step - loss: 1.0481 - acc: 0.6001 - val_loss: 1.1444 - val_acc: 0.5607\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.08037\n",
      "Epoch 19/110\n",
      "898/897 [==============================] - 174s 194ms/step - loss: 1.0400 - acc: 0.6038 - val_loss: 1.0768 - val_acc: 0.5932\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.08037 to 1.07681, saving model to models/_mini_XCEPTION.19-0.59.hdf5\n",
      "Epoch 20/110\n",
      "898/897 [==============================] - 173s 193ms/step - loss: 1.0401 - acc: 0.6033 - val_loss: 1.0778 - val_acc: 0.5982\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.07681\n",
      "Epoch 21/110\n",
      "898/897 [==============================] - 174s 193ms/step - loss: 1.0317 - acc: 0.6051 - val_loss: 1.0426 - val_acc: 0.6003\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.07681 to 1.04262, saving model to models/_mini_XCEPTION.21-0.60.hdf5\n",
      "Epoch 22/110\n",
      "898/897 [==============================] - 174s 194ms/step - loss: 1.0247 - acc: 0.6058 - val_loss: 1.1146 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.04262\n",
      "Epoch 23/110\n",
      "898/897 [==============================] - 180s 201ms/step - loss: 1.0203 - acc: 0.6080 - val_loss: 1.0520 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.04262\n",
      "Epoch 24/110\n",
      "898/897 [==============================] - 192s 214ms/step - loss: 1.0120 - acc: 0.6122 - val_loss: 1.0875 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.04262\n",
      "Epoch 25/110\n",
      "898/897 [==============================] - 179s 200ms/step - loss: 1.0032 - acc: 0.6179 - val_loss: 1.0841 - val_acc: 0.5843\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.04262\n",
      "Epoch 26/110\n",
      "898/897 [==============================] - 179s 199ms/step - loss: 1.0082 - acc: 0.6133 - val_loss: 1.0367 - val_acc: 0.6037\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.04262 to 1.03674, saving model to models/_mini_XCEPTION.26-0.60.hdf5\n",
      "Epoch 27/110\n",
      "898/897 [==============================] - 180s 201ms/step - loss: 0.9998 - acc: 0.6205 - val_loss: 1.0533 - val_acc: 0.5947\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.03674\n",
      "Epoch 28/110\n",
      "898/897 [==============================] - 180s 201ms/step - loss: 0.9958 - acc: 0.6233 - val_loss: 1.0955 - val_acc: 0.5908\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.03674\n",
      "Epoch 29/110\n",
      "898/897 [==============================] - 180s 201ms/step - loss: 0.9917 - acc: 0.6220 - val_loss: 1.0765 - val_acc: 0.5978\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.03674\n",
      "Epoch 30/110\n",
      "898/897 [==============================] - 181s 202ms/step - loss: 0.9855 - acc: 0.6250 - val_loss: 1.0421 - val_acc: 0.6103\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.03674\n",
      "Epoch 31/110\n",
      "898/897 [==============================] - 180s 201ms/step - loss: 0.9826 - acc: 0.6248 - val_loss: 1.0303 - val_acc: 0.6052\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.03674 to 1.03026, saving model to models/_mini_XCEPTION.31-0.61.hdf5\n",
      "Epoch 32/110\n",
      "898/897 [==============================] - 178s 198ms/step - loss: 0.9796 - acc: 0.6263 - val_loss: 0.9966 - val_acc: 0.6229\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.03026 to 0.99659, saving model to models/_mini_XCEPTION.32-0.62.hdf5\n",
      "Epoch 33/110\n",
      "898/897 [==============================] - 176s 196ms/step - loss: 0.9736 - acc: 0.6299 - val_loss: 1.0189 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.99659\n",
      "Epoch 34/110\n",
      "898/897 [==============================] - 180s 200ms/step - loss: 0.9703 - acc: 0.6283 - val_loss: 1.0416 - val_acc: 0.6069\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.99659\n",
      "Epoch 35/110\n",
      "898/897 [==============================] - 173s 193ms/step - loss: 0.9670 - acc: 0.6322 - val_loss: 1.0706 - val_acc: 0.5961\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.99659\n",
      "Epoch 36/110\n",
      "898/897 [==============================] - 173s 193ms/step - loss: 0.9682 - acc: 0.6315 - val_loss: 1.0028 - val_acc: 0.6176\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.99659\n",
      "Epoch 37/110\n",
      "898/897 [==============================] - 174s 193ms/step - loss: 0.9631 - acc: 0.6357 - val_loss: 1.0530 - val_acc: 0.6108\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.99659\n",
      "Epoch 38/110\n",
      "898/897 [==============================] - 174s 194ms/step - loss: 0.9588 - acc: 0.6348 - val_loss: 1.0651 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.99659\n",
      "Epoch 39/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.9551 - acc: 0.6390 - val_loss: 1.0697 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.99659\n",
      "Epoch 40/110\n",
      "898/897 [==============================] - 177s 197ms/step - loss: 0.9527 - acc: 0.6377 - val_loss: 1.0938 - val_acc: 0.5928\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.99659\n",
      "Epoch 41/110\n",
      "898/897 [==============================] - 176s 196ms/step - loss: 0.9509 - acc: 0.6397 - val_loss: 1.0485 - val_acc: 0.6013\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.99659\n",
      "Epoch 42/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.9482 - acc: 0.6411 - val_loss: 1.0551 - val_acc: 0.6063\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.99659\n",
      "Epoch 43/110\n",
      "898/897 [==============================] - 176s 196ms/step - loss: 0.9455 - acc: 0.6412 - val_loss: 1.0202 - val_acc: 0.6149\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.99659\n",
      "Epoch 44/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.9419 - acc: 0.6406 - val_loss: 1.0018 - val_acc: 0.6188\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.99659\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 45/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.8964 - acc: 0.6594 - val_loss: 0.9727 - val_acc: 0.6368\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.99659 to 0.97267, saving model to models/_mini_XCEPTION.45-0.64.hdf5\n",
      "Epoch 46/110\n",
      "898/897 [==============================] - 176s 196ms/step - loss: 0.8828 - acc: 0.6677 - val_loss: 0.9641 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.97267 to 0.96410, saving model to models/_mini_XCEPTION.46-0.64.hdf5\n",
      "Epoch 47/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.8744 - acc: 0.6709 - val_loss: 0.9643 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.96410\n",
      "Epoch 48/110\n",
      "898/897 [==============================] - 176s 196ms/step - loss: 0.8747 - acc: 0.6694 - val_loss: 0.9630 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.96410 to 0.96303, saving model to models/_mini_XCEPTION.48-0.64.hdf5\n",
      "Epoch 49/110\n",
      "898/897 [==============================] - 176s 196ms/step - loss: 0.8692 - acc: 0.6723 - val_loss: 0.9632 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.96303\n",
      "Epoch 50/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.8688 - acc: 0.6725 - val_loss: 0.9622 - val_acc: 0.6421\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.96303 to 0.96216, saving model to models/_mini_XCEPTION.50-0.64.hdf5\n",
      "Epoch 51/110\n",
      "898/897 [==============================] - 175s 194ms/step - loss: 0.8635 - acc: 0.6772 - val_loss: 0.9638 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.96216\n",
      "Epoch 52/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.8630 - acc: 0.6746 - val_loss: 0.9673 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.96216\n",
      "Epoch 53/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.8597 - acc: 0.6773 - val_loss: 0.9633 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.96216\n",
      "Epoch 54/110\n",
      "898/897 [==============================] - 175s 195ms/step - loss: 0.8546 - acc: 0.6766 - val_loss: 0.9654 - val_acc: 0.6402\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.96216\n",
      "Epoch 55/110\n",
      "898/897 [==============================] - 174s 194ms/step - loss: 0.8579 - acc: 0.6789 - val_loss: 0.9616 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.96216 to 0.96160, saving model to models/_mini_XCEPTION.55-0.64.hdf5\n",
      "Epoch 56/110\n",
      "898/897 [==============================] - 173s 193ms/step - loss: 0.8557 - acc: 0.6779 - val_loss: 0.9666 - val_acc: 0.6402\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.96160\n",
      "Epoch 57/110\n",
      "898/897 [==============================] - 176s 196ms/step - loss: 0.8583 - acc: 0.6778 - val_loss: 0.9693 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.96160\n",
      "Epoch 58/110\n",
      "898/897 [==============================] - 178s 199ms/step - loss: 0.8532 - acc: 0.6788 - val_loss: 0.9684 - val_acc: 0.6413\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.96160\n",
      "Epoch 59/110\n",
      "898/897 [==============================] - 183s 204ms/step - loss: 0.8527 - acc: 0.6789 - val_loss: 0.9739 - val_acc: 0.6358\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.96160\n",
      "Epoch 60/110\n",
      "898/897 [==============================] - 182s 203ms/step - loss: 0.8471 - acc: 0.6806 - val_loss: 0.9800 - val_acc: 0.6358\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.96160\n",
      "Epoch 61/110\n",
      "898/897 [==============================] - 183s 203ms/step - loss: 0.8518 - acc: 0.6793 - val_loss: 0.9648 - val_acc: 0.6362\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.96160\n",
      "Epoch 62/110\n",
      "898/897 [==============================] - 183s 204ms/step - loss: 0.8479 - acc: 0.6822 - val_loss: 0.9665 - val_acc: 0.6399\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.96160\n",
      "Epoch 63/110\n",
      "898/897 [==============================] - 182s 203ms/step - loss: 0.8459 - acc: 0.6818 - val_loss: 0.9776 - val_acc: 0.6356\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.96160\n",
      "Epoch 64/110\n",
      "898/897 [==============================] - 889s 990ms/step - loss: 0.8458 - acc: 0.6829 - val_loss: 0.9615 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.96160 to 0.96151, saving model to models/_mini_XCEPTION.64-0.64.hdf5\n",
      "Epoch 65/110\n",
      "898/897 [==============================] - 184s 205ms/step - loss: 0.8454 - acc: 0.6822 - val_loss: 0.9692 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.96151\n",
      "Epoch 66/110\n",
      "898/897 [==============================] - 183s 204ms/step - loss: 0.8457 - acc: 0.6834 - val_loss: 0.9586 - val_acc: 0.6393\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.96151 to 0.95857, saving model to models/_mini_XCEPTION.66-0.64.hdf5\n",
      "Epoch 67/110\n",
      "898/897 [==============================] - 184s 205ms/step - loss: 0.8410 - acc: 0.6819 - val_loss: 0.9679 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.95857\n",
      "Epoch 68/110\n",
      "898/897 [==============================] - 185s 206ms/step - loss: 0.8413 - acc: 0.6841 - val_loss: 0.9636 - val_acc: 0.6411\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.95857\n",
      "Epoch 69/110\n",
      "898/897 [==============================] - 1180s 1s/step - loss: 0.8396 - acc: 0.6852 - val_loss: 0.9610 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.95857\n",
      "Epoch 70/110\n",
      "898/897 [==============================] - 159s 178ms/step - loss: 0.8408 - acc: 0.6821 - val_loss: 0.9650 - val_acc: 0.6407\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.95857\n",
      "Epoch 71/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8411 - acc: 0.6825 - val_loss: 0.9669 - val_acc: 0.6410\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.95857\n",
      "Epoch 72/110\n",
      "898/897 [==============================] - 158s 176ms/step - loss: 0.8396 - acc: 0.6849 - val_loss: 0.9630 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.95857\n",
      "Epoch 73/110\n",
      "898/897 [==============================] - 158s 176ms/step - loss: 0.8330 - acc: 0.6857 - val_loss: 0.9568 - val_acc: 0.6445\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.95857 to 0.95679, saving model to models/_mini_XCEPTION.73-0.64.hdf5\n",
      "Epoch 74/110\n",
      "898/897 [==============================] - 158s 176ms/step - loss: 0.8383 - acc: 0.6846 - val_loss: 0.9621 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.95679\n",
      "Epoch 75/110\n",
      "898/897 [==============================] - 158s 176ms/step - loss: 0.8382 - acc: 0.6839 - val_loss: 0.9543 - val_acc: 0.6457\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.95679 to 0.95429, saving model to models/_mini_XCEPTION.75-0.65.hdf5\n",
      "Epoch 76/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8370 - acc: 0.6850 - val_loss: 0.9604 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.95429\n",
      "Epoch 77/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8369 - acc: 0.6856 - val_loss: 0.9614 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.95429\n",
      "Epoch 78/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8346 - acc: 0.6846 - val_loss: 0.9640 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.95429\n",
      "Epoch 79/110\n",
      "898/897 [==============================] - 160s 178ms/step - loss: 0.8351 - acc: 0.6867 - val_loss: 0.9617 - val_acc: 0.6411\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.95429\n",
      "Epoch 80/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/897 [==============================] - 158s 176ms/step - loss: 0.8329 - acc: 0.6887 - val_loss: 0.9614 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.95429\n",
      "Epoch 81/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8304 - acc: 0.6867 - val_loss: 0.9679 - val_acc: 0.6383\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.95429\n",
      "Epoch 82/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8318 - acc: 0.6877 - val_loss: 0.9614 - val_acc: 0.6421\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.95429\n",
      "Epoch 83/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8320 - acc: 0.6862 - val_loss: 0.9630 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.95429\n",
      "Epoch 84/110\n",
      "898/897 [==============================] - 159s 177ms/step - loss: 0.8331 - acc: 0.6843 - val_loss: 0.9569 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.95429\n",
      "Epoch 85/110\n",
      "898/897 [==============================] - 158s 177ms/step - loss: 0.8307 - acc: 0.6889 - val_loss: 0.9526 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.95429 to 0.95257, saving model to models/_mini_XCEPTION.85-0.65.hdf5\n",
      "Epoch 86/110\n",
      "898/897 [==============================] - 25450s 28s/step - loss: 0.8268 - acc: 0.6870 - val_loss: 0.9626 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.95257\n",
      "Epoch 87/110\n",
      "898/897 [==============================] - 161s 180ms/step - loss: 0.8292 - acc: 0.6877 - val_loss: 0.9534 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.95257\n",
      "Epoch 88/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8287 - acc: 0.6883 - val_loss: 0.9538 - val_acc: 0.6478\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.95257\n",
      "Epoch 89/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8227 - acc: 0.6880 - val_loss: 0.9577 - val_acc: 0.6459\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.95257\n",
      "Epoch 90/110\n",
      "898/897 [==============================] - 154s 172ms/step - loss: 0.8264 - acc: 0.6868 - val_loss: 0.9589 - val_acc: 0.6410\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.95257\n",
      "Epoch 91/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8264 - acc: 0.6883 - val_loss: 0.9620 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.95257\n",
      "Epoch 92/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8267 - acc: 0.6870 - val_loss: 0.9604 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.95257\n",
      "Epoch 93/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8254 - acc: 0.6897 - val_loss: 0.9568 - val_acc: 0.6457\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.95257\n",
      "Epoch 94/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8254 - acc: 0.6883 - val_loss: 0.9589 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.95257\n",
      "Epoch 95/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8228 - acc: 0.6864 - val_loss: 0.9660 - val_acc: 0.6435\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.95257\n",
      "Epoch 96/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8199 - acc: 0.6927 - val_loss: 0.9674 - val_acc: 0.6393\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.95257\n",
      "Epoch 97/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8224 - acc: 0.6901 - val_loss: 0.9643 - val_acc: 0.6404\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.95257\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 98/110\n",
      "898/897 [==============================] - 156s 173ms/step - loss: 0.8193 - acc: 0.6919 - val_loss: 0.9563 - val_acc: 0.6428\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.95257\n",
      "Epoch 99/110\n",
      "898/897 [==============================] - 157s 175ms/step - loss: 0.8130 - acc: 0.6941 - val_loss: 0.9549 - val_acc: 0.6435\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.95257\n",
      "Epoch 100/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8176 - acc: 0.6887 - val_loss: 0.9562 - val_acc: 0.6427\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.95257\n",
      "Epoch 101/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8123 - acc: 0.6965 - val_loss: 0.9556 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.95257\n",
      "Epoch 102/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8154 - acc: 0.6940 - val_loss: 0.9550 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.95257\n",
      "Epoch 103/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8146 - acc: 0.6939 - val_loss: 0.9552 - val_acc: 0.6428\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.95257\n",
      "Epoch 104/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8165 - acc: 0.6941 - val_loss: 0.9551 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.95257\n",
      "Epoch 105/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8136 - acc: 0.6931 - val_loss: 0.9545 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.95257\n",
      "Epoch 106/110\n",
      "898/897 [==============================] - 154s 172ms/step - loss: 0.8131 - acc: 0.6952 - val_loss: 0.9554 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.95257\n",
      "Epoch 107/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8147 - acc: 0.6939 - val_loss: 0.9560 - val_acc: 0.6439\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.95257\n",
      "Epoch 108/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8159 - acc: 0.6918 - val_loss: 0.9565 - val_acc: 0.6436\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.95257\n",
      "Epoch 109/110\n",
      "898/897 [==============================] - 155s 172ms/step - loss: 0.8146 - acc: 0.6949 - val_loss: 0.9573 - val_acc: 0.6431\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.95257\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 110/110\n",
      "898/897 [==============================] - 155s 173ms/step - loss: 0.8133 - acc: 0.6921 - val_loss: 0.9559 - val_acc: 0.6422\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.95257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14d2c94a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    " \n",
    "# model parameters\n",
    "regularization = l2(l2_regularization)\n",
    " \n",
    "# base\n",
    "img_input = Input(input_shape)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    " \n",
    "# module 1\n",
    "residual = Conv2D(16, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    " \n",
    "# module 2\n",
    "residual = Conv2D(32, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    " \n",
    "# module 3\n",
    "residual = Conv2D(64, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    " \n",
    "# module 4\n",
    "residual = Conv2D(128, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "x = Conv2D(num_classes, (3, 3), padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Activation('softmax',name='predictions')(x)\n",
    " \n",
    "model = Model(img_input, output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    " \n",
    "# callbacks\n",
    "log_file_path = base_path + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    " \n",
    "model.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),\n",
    "                        steps_per_epoch=len(xtrain) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import imutils\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    " \n",
    "# parameters for loading data and images\n",
    "detection_model_path = '/Users/kcl759/MLProject/haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = '/Users/kcl759/MLProject/models/_mini_XCEPTION.85-0.65.hdf5'\n",
    "img_path = '/Users/kcl759/MLProject/test_images/angry_1.jpg'\n",
    " \n",
    "# hyper-parameters for bounding boxes shape\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\",\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\"neutral\"]\n",
    " #reading the frame\n",
    "orig_frame = cv2.imread(img_path) \n",
    "frame = cv2.imread(img_path,0)\n",
    "faces = face_detection.detectMultiScale(frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "if len(faces) > 0:\n",
    "    faces = sorted(faces, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "    (fX, fY, fW, fH) = faces\n",
    "    roi = frame[fY:fY + fH, fX:fX + fW]\n",
    "    roi = cv2.resize(roi, (48, 48))\n",
    "    roi = roi.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "    preds = emotion_classifier.predict(roi)[0]\n",
    "    emotion_probability = np.max(preds)\n",
    "    label = EMOTIONS[preds.argmax()]\n",
    "    cv2.putText(orig_frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    cv2.rectangle(orig_frame, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
    "    \n",
    "cv2.imshow('test_face', orig_frame)\n",
    "cv2.imwrite('/Users/kcl759/MLProject/test_output/'+img_path.split('/')[-1],orig_frame)\n",
    "if (cv2.waitKey(2000) & 0xFF == ord('q')):\n",
    "    sys.exit(\"Thanks\")\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
